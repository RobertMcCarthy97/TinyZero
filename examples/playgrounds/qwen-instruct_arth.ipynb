{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f6758cc75f433da5db72835056662d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "\n",
    "The user will ask you to solve a problem about a series of coin flips. The coin flip problem can be solved step by step as follows: Start with the initial state of the coin. Then, for each flip, track the new state of the coin after the flip.\n",
    "\n",
    "You perform you reasoning in <think> </think> tags and then provide your final answer in <answer> </answer> tags; e.g., <think> reasoning process tracking the state of the coin here </think> <answer> Heads/Tails </answer>.<|im_end|>\n",
    "<|im_start|>user\n",
    "The coin starts on Tails. It is flipped 6 times. The results of these flips are: \n",
    "- Flip 1: Same side as previous, \n",
    "- Flip 2: Different side to previous, \n",
    "- Flip 3: Same side as previous, \n",
    "- Flip 4: Same side as previous, \n",
    "- Flip 5: Different side to previous, \n",
    "- Flip 6: Different side to previous.\n",
    "\n",
    "What side did the coin land on after the final flip?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Let me solve this step by step.\n",
    "<think>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "\n",
    "The user will ask you a multiplication question. You will solve the problem step-by-step by breaking down the multiplication using the distributive property: Separating larger numbers into more manageable parts, computing the partial products separately, then adding the partial products to obtain the final result.\n",
    "\n",
    "You will return the final answer in <answer> </answer> tags, for example <answer> RESULT_NUMBER </answer>.\n",
    "\n",
    "You will be penalized for using the following symbols: \\\"*\\\", \\\"+\\\", \\\"-\\\", \\\"=\\\". You should replace these symbols with something else during your calculations.<|im_end|>\n",
    "<|im_start|>user\n",
    "Give me the answer of the following equation: 976 * 154.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Let me solve this step by step.\n",
    "<think>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.\n",
      "\n",
      "The user will ask you to solve a problem about a series of coin flips. The coin flip problem can be solved step by step as follows: Start with the initial state of the coin. Then, for each flip, track the new state of the coin after the flip.\n",
      "\n",
      "You perform you reasoning in <think> </think> tags and then provide your final answer in <answer> </answer> tags; e.g., <think> reasoning process tracking the state of the coin here </think> <answer> Heads/Tails </answer>.<|im_end|>\n",
      "<|im_start|>user\n",
      "The coin starts on Tails. It is flipped 6 times. The results of these flips are: \n",
      "- Flip 1: Same side as previous, \n",
      "- Flip 2: Different side to previous, \n",
      "- Flip 3: Same side as previous, \n",
      "- Flip 4: Same side as previous, \n",
      "- Flip 5: Different side to previous, \n",
      "- Flip 6: Different side to previous.\n",
      "\n",
      "What side did the coin land on after the final flip?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let me solve this step by step.\n",
      "<think>\n",
      "\n",
      " We start with the coin on Tails. Let's track the outcomes based on the given rules:\n",
      "1. Flip 1: Same side as previous → No change (Tails)\n",
      "2. Flip 2: Different side to previous → Heads (from Tails)\n",
      "3. Flip 3: Same side as previous → Heads (from Heads)\n",
      "4. Flip 4: Same side as previous → Heads (from Heads)\n",
      "5. Flip 5: Different side to previous → Tails (from Heads)\n",
      "6. Flip 6: Different side to previous → Heads (from Tails)\n",
      "</think>\n",
      "<answer> Heads </answer>\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print()\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tiny_zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
