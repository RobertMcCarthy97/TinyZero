{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6ec8f17be34f72bea29336d505916c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1381b61f8f04432b29c24315bd8cd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c700a23f31342c19995716271998ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3d9c62715b4b8293d0a93840ba06c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedc182dd9b549e28e3a2ba1481aa11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/683 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01d7ab8511245a495072a8622aaac97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541f379b645b4117a588861de75b5ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venvs/.tiny_zero/lib/python3.10/site-packages/huggingface_hub/file_download.py:651: UserWarning: Not enough free disk space to download the file. The expected file size is: 3968.66 MB. The target location /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B/blobs only has 3374.69 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c857739fad474a5c88e15b85e3ba8a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = \"rmcc11/arth_default_qwen3B-actor-latest\"\n",
    "model_name = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1 = \"323\"\n",
    "op = \"*\"\n",
    "num2 = \"68\"\n",
    "\n",
    "initial_prompt_standard = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> RESULT_NUMBER </answer>. \\nUser: Give me the answer of the following equation: {num1} {op} {num2}.\\nAssistant: Let me solve this step by step.\\n<think>\"\"\"\n",
    "\n",
    "initial_prompt_replace = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. When performing calculations, the assistant replaces mathematical symbols with words: '*' -> 'by', '-' -> 'minus', '+' -> 'plus', and '=' -> 'equals'. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> RESULT_NUMBER </answer>. \\nUser: Give me the answer of the following equation: {num1} {op} {num2}.\\nAssistant: Let me solve this step by step.\\n<think>\"\"\"\n",
    "\n",
    "initial_prompt_replace_vague = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. When performing calculations, the assistant replaces mathematical symbols (e.g., '*', '-', '+', '=') with something else. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> RESULT_NUMBER </answer>. \\nUser: Give me the answer of the following equation: {num1} {op} {num2}.\\nAssistant: Let me solve this step by step.\\n<think>\"\"\"\n",
    "\n",
    "completion = \"To find the product of 323 and 68, we can use the distributive property: 323 * 68 = 323 * (60 + 8) = (323 * 60) + (323 \"\n",
    "completion_initial = \"To find the product of 323 and 68, we can use the distributive property: 323 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_single_tokens(prompt: str, n: int, temperature: float = 0.7) -> list:\n",
    "    \"\"\"Generate N single tokens from a prompt using specified temperature.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    generated_tokens = []\n",
    "    \n",
    "    for _ in range(n):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "            \n",
    "            new_token = outputs[0][len(inputs['input_ids'][0]):]\n",
    "            decoded_token = tokenizer.decode(new_token)\n",
    "            generated_tokens.append(decoded_token)\n",
    "    \n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_single_tokens_batched(prompt: str, n: int, batch_size: int = 32, temperature: float = 0.7) -> list:\n",
    "    \"\"\"Generate N single tokens from a prompt using specified temperature and batch processing.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt to generate tokens from\n",
    "        n: Number of tokens to generate\n",
    "        batch_size: Number of tokens to generate in parallel\n",
    "        temperature: Sampling temperature\n",
    "        \n",
    "    Returns:\n",
    "        List of generated tokens as strings\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    generated_tokens = []\n",
    "    \n",
    "    # Calculate number of full batches and remaining tokens\n",
    "    num_full_batches = n // batch_size\n",
    "    remaining_tokens = n % batch_size\n",
    "    \n",
    "    # Process full batches\n",
    "    for _ in range(num_full_batches):\n",
    "        with torch.no_grad():\n",
    "            # Expand input tensors to match batch size\n",
    "            batch_inputs = {\n",
    "                k: v.expand(batch_size, -1) for k, v in inputs.items()\n",
    "            }\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **batch_inputs,\n",
    "                max_new_tokens=1,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_return_sequences=batch_size\n",
    "            )\n",
    "            \n",
    "            # Extract and decode new tokens from each sequence\n",
    "            for seq in outputs:\n",
    "                new_token = seq[len(inputs['input_ids'][0]):]\n",
    "                decoded_token = tokenizer.decode(new_token)\n",
    "                generated_tokens.append(decoded_token)\n",
    "    \n",
    "    # Process remaining tokens if any\n",
    "    if remaining_tokens > 0:\n",
    "        with torch.no_grad():\n",
    "            batch_inputs = {\n",
    "                k: v.expand(remaining_tokens, -1) for k, v in inputs.items()\n",
    "            }\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **batch_inputs,\n",
    "                max_new_tokens=1,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_return_sequences=remaining_tokens\n",
    "            )\n",
    "            \n",
    "            for seq in outputs:\n",
    "                new_token = seq[len(inputs['input_ids'][0]):]\n",
    "                decoded_token = tokenizer.decode(new_token)\n",
    "                generated_tokens.append(decoded_token)\n",
    "    \n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      "6\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      "6\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      "6\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      "6\n",
      "6\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      "6\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      "6\n",
      " *\n",
      " *\n",
      "6\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n",
      "6\n",
      " *\n",
      " *\n",
      " *\n",
      " *\n"
     ]
    }
   ],
   "source": [
    "prompt = initial_prompt_standard + completion\n",
    "n_generations = 200\n",
    "temperature = 1.0\n",
    "batch_size = 4\n",
    "\n",
    "# Example usage\n",
    "tokens = get_n_single_tokens(prompt, n=n_generations, temperature=temperature)\n",
    "# tokens = get_n_single_tokens_batched(prompt, n=n_generations, temperature=temperature, batch_size=batch_size)\n",
    "print(\"Generated tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 1.0\n",
      "Total tokens generated: 200\n",
      "Number of tokens containing valid strings: 191\n",
      "\n",
      "Counts per valid string:\n",
      "'*': 191\n",
      "'x': 0\n",
      "'by': 0\n",
      "'time': 0\n",
      "'mul': 0\n",
      "\n",
      "Top 5 tokens by occurrence:\n",
      "' *': 191\n",
      "'6': 9\n"
     ]
    }
   ],
   "source": [
    "valid_strings_to_check_for = [\"*\", \"x\", \"by\", \"time\", \"mul\"]\n",
    "\n",
    "# Count tokens containing valid strings\n",
    "token_counts = {s: sum(1 for t in tokens if s in t) for s in valid_strings_to_check_for}\n",
    "total_valid_tokens = sum(1 for t in tokens if any(s in t for s in valid_strings_to_check_for))\n",
    "\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Total tokens generated: {len(tokens)}\")\n",
    "print(f\"Number of tokens containing valid strings: {total_valid_tokens}\")\n",
    "print(\"\\nCounts per valid string:\")\n",
    "for string, count in token_counts.items():\n",
    "    print(f\"'{string}': {count}\")\n",
    "\n",
    "# Count all unique tokens and get top 5 by occurrence\n",
    "from collections import Counter\n",
    "token_counter = Counter(tokens)\n",
    "print(\"\\nTop 5 tokens by occurrence:\")\n",
    "for token, count in token_counter.most_common(5):\n",
    "    print(f\"'{token}': {count}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tiny_zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
