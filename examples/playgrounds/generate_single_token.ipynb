{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1 = \"323\"\n",
    "op = \"*\"\n",
    "num2 = \"68\"\n",
    "\n",
    "initial_prompt = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> RESULT_NUMBER </answer>. \\nUser: Give me the answer of the following equation: {num1} {op} {num2}.\\nAssistant: Let me solve this step by step.\\n<think>\"\"\"\n",
    "\n",
    "completion = \"To find the product of 323 and 68, we can use the distributive property: 323 * 68 = 323 * (60 + 8) = (323 * 60) + (323 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_single_tokens(prompt: str, n: int, temperature: float = 0.7) -> list:\n",
    "    \"\"\"Generate N single tokens from a prompt using specified temperature.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    generated_tokens = []\n",
    "    \n",
    "    for _ in range(n):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "            \n",
    "            new_token = outputs[0][len(inputs['input_ids'][0]):]\n",
    "            decoded_token = tokenizer.decode(new_token)\n",
    "            generated_tokens.append(decoded_token)\n",
    "    \n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = initial_prompt + completion\n",
    "n_generations = 50\n",
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      "5\n",
      "8\n",
      " \n",
      "8\n",
      " \n",
      " \n",
      "8\n",
      "0\n",
      "8\n",
      "8\n",
      "3\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      " *\n",
      " which\n",
      "1\n",
      "8\n",
      " =\n",
      "3\n",
      "8\n",
      "3\n",
      "8\n",
      "8\n",
      " times\n",
      "1\n",
      "8\n",
      "8\n",
      "8\n",
      " *\n",
      "8\n",
      "8\n",
      "8\n",
      "3\n",
      "3\n",
      "3\n",
      "8\n",
      "3\n",
      "8\n",
      "8\n",
      "3\n",
      " *\n",
      "3\n",
      "8\n",
      "8\n",
      " *\n",
      "2\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tokens = get_n_single_tokens(prompt, n=n_generations, temperature=temperature)\n",
    "print(\"Generated tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tiny_zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
